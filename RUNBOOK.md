This runbook describes how to respond to alerts generated by the Nginx alert watcher service monitoring the Blue/Green deployment.

# Alert Types

## Failover Detected
- Traffic has automatically switched from one pool (Blue/Green) to another due to upstream failures or health check issues.

### Possible root causes:
- Container crash 
- Application hanging 
- Network issues
- Health check endpoint failing
- Resource exhaustion (CPU/memory)

### Response steps
- Check the failed pool's health
- Inspect container resource usage
- Test the failed pool directly
- Check application logs for errors
- If container is down, restart it
- Monitor for recovery alert

## High Error Rate Alert
- The upstream servers are returning elevated 5xx errors, exceeding the configured threshold over a sliding window of requests.

### Possible root causes:
- Application bugs or crashes
- Database connection issues
- External API failures
- Resource exhaustion
- Configuration errors
- Code deployment issues

### Response steps
- Identify which pool is experiencing errors
- Check application logs for the current pool
- Verify backend dependencies
- Check resource constraints
- Consider manual pool toggle if errors persist
- If both pools are affected

## Recovery Detected
- The previously failed pool is healthy again and is now serving traffic.

### Response steps
- Verify recovery
- Review what caused the initial failure
- Update incident documentation

# Escalation
If alerts persist or you cannot resolve the issue:

## Immediate Response:
- Toggle to the healthy pool if one exists
- Scale up resources if needed
- Roll back recent deployments


## Escalation Path:
- Level 1: On-call engineer (check logs, restart containers)
- Level 2: Development team (investigate application errors)
- Level 3: Infrastructure team (check infrastructure, networking)

## Documentation:
- Save relevant logs
- Screenshot alerts
- Document timeline
- Create post-incident review
